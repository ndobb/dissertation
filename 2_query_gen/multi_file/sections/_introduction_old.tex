
\documentclass[../main.tex]{subfiles}

\begin{document}

\noindent Identifying groups of patients meeting a given set of eligibility criteria is a critical step for recruitment into randomized controlled trials (RCTs). Yet many trials  fall short of recruitment goals, leading to time and cost overruns while creating challenges in ensuring adequate statistical power \cite{gul2010clinical, adams2015barriers}. Failures in adequate recruitment may result from a variety of factors, but can often stem from difficulties in translating complex eligibility criteria into queries using data collected in the electronic medical record (EHR) \cite{wang2017classifying}. Despite these difficulties, RCTs increasingly rely on EHR data as a useful and more expedient means of identifying potential patients versus manual chart or case report form review \cite{cowie2017electronic}. At the same time, EHRs increasingly capture and store patient health and outcomes data in greater volume and variety, creating additional challenges and opportunities for patient recruitment \cite{lee2017medical}. While more granular - and potentially useful - data may be captured and stored in EHRs than in the past, the process of accessing and leveraging those data in the context of a given trial's eligibility criteria often require extensive technical expertise and knowledge of biomedical terminologies and data models. \\

\noindent Cohort discovery tools such as Leaf \cite{dobbins2019leaf} and i2b2 \cite{murphy2010serving} may be used in many cases, as they offer relatively simple drag-and-drop interfaces capable of querying EHR data to find patients meeting given criteria \cite{johnson2014use}. Yet these tools are not a panacea, as they often have significant learning curves and may be unable to represent particularly complex nested or temporal eligibility criteria \cite{deshmukh2009evaluating}. Moreover, existing cohort discovery tools lack functionality to dynamically reason upon non-specific criteria that frequently appear in real-world eligibility criteria. For example, a criterion may require patients "indicated for bariatric surgery", but translating such non-specific criteria into a query (e.g., patients with a diagnosis of morbid obesity or body mass index greater than 40) must be performed manually by a researcher, even in cases where constructing an exhaustive list of such criteria may be time-intensive, subjective, and error-prone.

\subsection*{Natural language processing approaches for cohort discovery}

\noindent In recent years, alternatives to web-based cohort discovery tools for RCT recruitment have also been explored. In particular, various methods using Natural Language Processing (NLP) have been put forth by the research community \cite{yuan2019criteria2query, soni2020patient, fang2022combining, zhang2020deepenroll, chen2019clinical, patrao2015recruit, dhayne2021emr2vec, liu2021evaluating, xiong2019cohort}. NLP-based cohort discovery methods hold unique potential and appeal, as they are hypothetically able to leverage existing eligibility criteria described in natural language, a medium researchers and investigators already use and are comfortable with. Recent methods which utilize NLP in some form can generally be grouped into 5 categories:

\begin{enumerate}
    \item{\textbf{Database query generation} to Structured Query Language (SQL) or similar systems using either (a) rules, (b) neural network-based encoder-decoder architectures}, or both.
    \item{\textbf{Document ranking and classification} using clinical notes in terms of relevancy vis-Ã -vis a given eligibility criteria.}
    \item{\textbf{Projection into embeddings} of patient medical history and trial eligibility criteria in a shared vector space and matching via similarity measurement or entailment.}
    \item{\textbf{Logical representations and reasoning} to represent eligibility criteria and patient records, matching by combinations of Semantic Web technologies, ontologies, Description Logics, and rule-based reasoning.}
    \item{A combination of the above.}
\end{enumerate}

\noindent Next we briefly describe recent relevant work in each category. \\

\noindent \textbf{Database query generation} - SQL-based relational databases are widely used both commercially and within academic institutions, and as such SQL is perhaps unsurprisingly often used as a target language in natural language to database query research \cite{dar2019frameworks}. Yuan \textit{et al} developed Criteria2Query, a hybrid information extraction (IE) pipeline and application which uses both rules and machine learning to generate database queries on an OMOP \cite{hripcsak2015observational} (Observation Medical Outcomes Partnership) database. This work was expanded by Fang \textit{et al}, who added functionality for iterative query generation via human correction and adjustment \cite{fang2022combining}. Although not specific to RCTs, other highly relevant recent work on query generation in the biomedical space has been done using encoder-decoder neural architectures for transforming clinical natural language questions into SQL queries \cite{bae2021question, park2021knowledge, wang2020text, pan2021bert, dhayne2021emr2vec}. Park \textit{et al} \cite{park2021knowledge} experimented with transforming medical questions generated in the MIMICSQL dataset \cite{wang2020text} using both SQL and SPARQL queries with varying database schema representations. Bae \textit{et al} similarly experimented with methods for handling typos, misspellings, and abbreviations in generating SQL queries from natural language questions. Pan \textit{et al} \cite{pan2021bert} leveraged intermediate abstract syntax tree-based representations and a SQL grammar-based decoder architecture for dynamic database schema matching. \\

\noindent \textbf{Document ranking and classification} - Focusing on clinical notes, Chen \textit{et al} \cite{chen2019clinical} used hybrid rule-based heuristics and sentence pattern-matching to detect criteria structure, as well as a combination of neural network-based bi-directional long short-term and conditional random field (biLSTM+CRF) architecture and knowledge graphs using the Unified Medical Language System (UMLS) for determining condition, lab, procedure and drug relationships. Soni and Roberts \cite{soni2020patient} utilized the BERT Transformer architecture \cite{devlin2018bert} and Lucene \cite{lucene} to summarize, rank and classify clinical notes as relevant to a given eligibility criterion, with the most relevant notes predicted to be eligible. \\

\noindent \textbf{Embedding projections} - Dhayne \textit{et al} \cite{dhayne2021emr2vec} experimented with treating patient-to-RCT matching as a joint embedding and similarity measurement problem while also incorporating the SNOMED-CT ontology to infer basic "is-a" and "has-type" relations between concepts. Similarly, Zhang \textit{et al} used joint patient and eligibility criteria embeddings for entailment prediction, where predicting that a patient can be inferred from a given eligibility criteria equates to eligibility. \\

\noindent \textbf{Logical representations and reasoning} - Patr{\~a}o \textit{et al} developed Recruit, an ontology-driven trial recruitment system which transformed SQL relational data to Resource Description Framework (RDF) graph-based triples. The RDF triples in turn were made query-able by use of an OWL-based reasoning system \cite{owl} and normalization techniques to infer cancer staging. Building upon earlier work \cite{patel2007matching, huang2013semanticct}, Baader \textit{et al} \cite{baader2018patient} explored the use of Description Logics and ontologies in matching patients in the MIMIC data set \cite{johnson2016mimic} to logical representations of eligibility criteria, for example representing "Diabetes mellitus type 1" as "$\exists_y$.diagnosed\_with(x, y) $\wedge$ Diabetes\_mellitus\_type\_1(y)".

\subsection*{Gaps and opportunities}

\noindent Much of the work to date on cohort discovery and RCT eligibility criteria matching has been impressive and impactful in leveraging advances in related research domains. Yet critical gaps remain in terms of practical usability, generalizability, and measurement of performance for these methods in the context of real-world clinical trials and data. \\

\noindent Among tools and methods for SQL query generation, for example, most efforts to date are capable of generating database queries on only a single database schema, such as OMOP or MIMIC. While the OMOP database schema is widely used in research, this lack of flexibility and adaptability toward other data models limits potential utility, in particular given the widely documented necessity to change and extend the standard OMOP schema to accommodate real-world project needs \cite{belenkaya2021extending, peng2021towards, zoch2021adaption, warner2019hemonc, zhou2013evaluation, shin2019genomic, kwon2019development}. Moreover, most methods for generating SQL queries, particularly those using encoder-decoder architectures, tend to generate relatively simple SQL statements, with few JOINs or nested sub-queries and typically no support for UNION operators and so on. This relative simplicity contrasts with the complexity of real-world clinical databases and it is not clear whether such methods alone are yet viable in an RCT context. \\

\noindent Methods utilizing clinical notes for document ranking show great potential, particularly given the significant amount of untapped information present in free-text as compared to structured data \cite{warrer2012using}. However the use of these systems at enterprise scale or as ad hoc query tools for researchers has been limited, and the number of notes used in these experiments tend to be few (hundreds or low thousands) as compared to the tens of millions of clinical notes stored within many EHRs. \\

\noindent Efforts using patient and eligibility criteria embeddings, while novel and showing future potential, also have notable limitations: (1) Zhang \textit{et al} assumed that patients who did not enroll in a given trial were not eligible (which may not necessarily be true), and (2) none of the research we are aware of provided sufficient detail on how structured data were transformed into embeddings or what specific data elements (e.g., diagnosis codes, labs) were used, preventing direct reproducibility.  Research in methods using Description Logics and related representations and ontologies, meanwhile, has been largely experimental and untested using large real-world clinical databases. Many works also require domain experts to first manually translate eligibility criteria to logical representations, making them somewhat unrealistic as cohort discovery tools. \\

\noindent Last, few of the methods described provide support for complex logic, and none support reasoning on non-specific criteria (e.g., "diseases that affect respiratory function"), two phenomena common to eligibility criteria \cite{wang2017classifying, ross2010analysis}. Perhaps most importantly, to the best of our knowledge, only one previous work has been tested in terms of matching patients enrolled in actual clinical trials \cite{zhang2020deepenroll} (with caveats discussed earlier), and none have been directly compared to the capabilities of a human database programmer. 

\subsection*{Key Contributions}

\noindent We introduce the LeafAI query engine, an application capable of generating database queries for cohort discovery from free-text eligibility descriptions. This work contributes the following:

\begin{enumerate}
    \item{A novel database schema annotation and mapping method to enable \textbf{data model-agnostic query generation from natural language.}}
    \item{Methods for transforming and leveraging \textbf{intermediate logical representations} of eligibility criteria.}
    \item{A \textbf{corpus of human-annotated eligibility criteria and corresponding logical representations} available to the research community\footnote{Will be made available upon article acceptance}.}
    \item{Methods for dynamically \textbf{reasoning upon non-specific criteria} using an integrated knowledge base of biomedical concepts.}
\end{enumerate}

\noindent We measure performance of the LeafAI query engine by comparing patients found by generated queries to those enrolled in actual past clinical trials using LeafAI and a human database programmer.

\end{document}