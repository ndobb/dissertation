 
\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection*{System Architecture}

The LeafAI query engine was designed using a modular, micro service-based architecture with a central API (Application Program Interface) which orchestrates end-to-end query generation. Inter-module communication is performed using gRPC \cite{grpc}, a robust open-source remote procedure call framework which enables language-agnostic service integration. This allows individual modules to be implemented (and substituted) in programming languages and using libraries well-suited to a given task. A diagram of the LeafAI query engine architecture is shown in Figure \ref{fig_leafai_architecture}. 

\begin{figure}[h]
  \includegraphics[scale=0.6]{figures/leafai_architecture.pdf}  
\caption{LeafAI query architecture. Inter-module communication is performed using the gRPC framework. Individual modules are deployed as Docker \cite{docker} containers and communicate solely with the central API, which orchestrates query generation and handles query generation requests.}
\label{fig_leafai_architecture}
\end{figure}

At a high level, query generation is performed in the following steps:

\begin{enumerate}
    \item{A query request is received by the API in the form of inclusion and exclusion criteria as free-text strings.}
    \item{The input texts are tokenized and named entity recognition is performed to determine spans of text representing conditions, procedures, and so on.}
    \item{Relation extraction is performed to determine relations between the entities, such as \textit{Caused-By} or \textit{Numeric-Filter}.}
    \item{The input texts are transformed by replacing spans of "raw" text with logical form names. For example, "Diagnosed with diabetes" would become "Diagnosed with cond("diabetes")." The resulting input texts are in turn transformed into an output logical representation using a Sequence to Sequence (Seq2Seq) architecture, in the form of a string.}
    \item{A logical form interpreter module implemented as a recursive descent parser \cite{johnstone1998generalised} reads the logical form string input and instantiates it as an abstract syntax tree (AST) of nested in-memory logical form objects.}
    \item{"Named" logical form objects (i.e., specified with quoted text, such as "lab("hemoglobin A1c")") are normalized into one or more corresponding UMLS concepts.}
    \item{Working recursively inside-to-outside the AST structure, each logical form object calls a \textit{Reason()} method which executes various rules depending on context.}
    \item{Each reasoning rule is performed as one or more pre-defined SPARQL queries to the knowledge base (KB), concept by concept.}
    \item{The final normalized, reasoned, logical form AST is thus a nested structure of UMLS concepts. Each AST criterion is mapped to zero or more corresponding entries in the semantic metadata mapping (SMM), which in turn lists meanings, roles, and relations of a database schema in the form of UMLS concepts.}
    \item{The final mapped AST object is transformed into a series of database queries, one per line of eligibility criteria text. The output SQL query can either be executed directly on a database or returned to the API caller.}
\end{enumerate}

\noindent Figure \ref{fig_leafai_querygen} illustrates an example of this process. In the following subsections we examine these steps in detail.

\begin{figure}[h]
  \includegraphics[scale=0.47]{figures/leafai_flow.pdf}  
\caption{LeafAI query generation processes}
\label{fig_leafai_querygen}
\end{figure}

\subsection*{Named entity recognition and relation extraction}

\noindent Named entity recognition (NER) refers to the segmentation and identification of tokens within an input sentence as "entities", such as conditions or procedures. We used the Leaf Clinical Trials (LCT) corpus \cite{dobbins2022leaf} to train two BERT-based \cite{devlin2018bert} NER extractors, one each for LCT general- and fine-grained-entities (see \cite{dobbins2022leaf} for more information on LCT entity types). Next, we perform relation extraction between named entity pairs similarly using a BERT-based model also trained on the LCT corpus.

\subsection*{Intermediate logical form transformation}

\noindent One of the core challenges of generating queries for eligibility criteria is the problem of logical representation. Generating queries directly based on named entities and relations, while practical, may perform poorly in cases of nested or complex logic. An alternative to this approach is to use a so-called intermediate representation (IR), which transforms the original natural language input by removing "noise" unnecessary to a given task and which more logically represents underlying semantics (see Herzig \textit{et al} \cite{herzig2021unlocking} for an examination of IR-based SQL generation approaches). Similar to earlier discussed work using Description Logics, Roberts and Demner-Fushman \cite{roberts2016annotating} proposed a representation of questions on EHR databases using a comparatively compact but flexible format using first order logic expressions, for example, representing "Is she wheezing this morning?" as

\begin{quote}
    \centering
    $\delta( \lambda x.has\_problem(x, C0043144, status) \wedge time\_within(x, \mathrm{"this\ morning"}))$
\end{quote}

\noindent This style of representation is powerfully generalizable, but also difficult to translate directly into SQL statements as multiple predicates (e.g., \textit{has\_problem} and \textit{time\_within}) may actually correspond to one or many SQL statements, depending on context, complicating direct transformation into queries.

We thus chose a similar intermediate representation (hereafter simply "logical forms") as proposed by Roberts and Demner-Fushman but more closely resembling a nested functional structure in programming languages such as Python or JavaScript and more amenable to SQL generation. A criterion such as "Diabetic women and men over age 65" would be represented by our logical forms as

\begin{quote}
$intersect( \\
    \mathrm{\ \ \ \ }cond("Diabetic"), \\
    \mathrm{\ \ \ \ }union(female(), male()),\\
    \mathrm{\ \ \ \ }age().num\_filter(eq(op(GT), val("65"))) \\
)$
\end{quote}

\noindent A description of our logical forms annotation schema, corpus, annotation process, and performance metrics can be found in Appendix A.

After named entity recognition and relation extraction are performed, we leverage T5 \cite{raffel2020exploring}, a state-of-the-art Seq2Seq architecture we fine-tuned for predicting logical forms. As inputs to the Seq2Seq model we use the original eligibility criteria with named entity spans replaced by logical form representations, as we found this to significantly improve performance compared to training with raw inputs without named entities. Thus the above example would be transformed to the input string

\begin{quote}
    \centering
    $\textit{"cond(“Diabetic”) female() and male() over age() eq(op(GT), val(“65”))"}$
\end{quote}

\noindent The returned logical form string is then instantiated into an abstract syntax tree (AST) of nested in-memory logical form objects using a recursive descent parser \cite{johnstone1998generalised} within the API.

\subsection*{Concept normalization}

Normalization refers to the mapping of free-text string values (e.g., "diabetes mellitus") to coded representations (e.g., UMLS, ICD-10, SNOMED, or LOINC). We normalize "named" logical forms to UMLS concepts using MetaMapLite \cite{aronson2001effective, demner2017metamap}. We consider a logical form "named" if it contains a free-text value surrounding by quotes. For example, \textit{cond()} is unnamed and refers to any condition or disease, while \textit{cond("hypertension")} is named as it refers to a specific condition. 

Normalization using MetaMapLite can often result in high recall but low precision, as MetaMapLite has no NER component and tends to return UMLS concepts which match a given phrase syntactically but refer to abstract concepts not of interest (e.g., a search for "BMI" may return "body mass index" (C1305855), but also organic chemical "BMI 60" (C0910133), likely unhelpful for query generation). To improve normalization precision, we employ two strategies. First, we compare term frequency-inverse document frequency (tf-idf) on MetaMapLite predictions, dropping UMLS concepts whose matched spans have a tf-idf score lower than that of unmatched spans in a given named entity. For example, for the string "covid-19 infection", MetaMapLite predicts both "COVID-19" (C5203670) as well as several concepts related to general infections. We pre-computed term-frequency across the entirety of the UMLS. Using our tf-idf strategy removes the erroneous infection concepts. Next, our NER component also us to further improve precision by filtering the predicted UMLS concepts to only those of specific semantic types. For example, we limit condition concepts to only those which include semantic types of signs or symptoms, diseases or syndromes, and so on.

Laboratory values present a particular challenge, as LeafAI expects predicted lab concepts to have directly associated LOINC codes, while MetaMapLite typically normalizes lab test strings to UMLS concepts of semantic type "laboratory test or finding", but which do not have direct mappings to LOINC codes. For example, a search for "platelet count" with MetaMapLite returns the concept "Platelet Count Measurement" (C0032181), but not the needed concept of "Platelet \# Bld Auto" (C0362994). Thus similar to Lee and Uzuner with medications \cite{lee2020normalizing}, we trained a BERT model specifically for normalization of lab tests.

\subsection*{Reasoning using an integrated knowledge base}

For reasoning and derivation of ICD-10, LOINC, and other codes for UMLS concepts, we designed a knowledge base (KB) accessible via SPARQL queries and stored as RDF triples. The core of our KB is the UMLS, derived using a variation of techniques created for ontologies in BioPortal \cite{noy2009bioportal}. To further augment the UMLS, we mapped and integrated the Disease Ontology \cite{schriml2012disease}, Symptom Ontology \cite{sayers2010database}, COVID-19 Ontology \cite{sargsyan2020covid}, Potential Drug-Drug Interactions \cite{ayvaz2015toward}, LOINC2HPO \cite{zhang2019semantic}, and the Disease-Symptom Knowledge Base \cite{wang2008automated}. We then developed SPARQL queries parameterized by UMLS concepts for various scenarios which leveraged our KB, such as contraindications to treatments, symptoms of diseases, and so on. Using LOINC2HPO mappings further allows us to infer phenotypes by lab test results rather than only ICD-10 or SNOMED codes, for example. 

Our KB, nested logical forms, and inside-to-outside normalization methods enable "multi-hop" reasoning on eligibility criteria over several steps. For example, given the non-specific criterion "Contraindications to drugs for conditions which affect respiratory function", our system successfully reasons that (among other results),

\begin{enumerate}
    \item \textbf{Asthma} causes changes to \textbf{respiratory function}
    \item \textbf{Methylprednisolone} can be used to treat \textbf{asthma}
    \item \textbf{Mycosis} (fungal infection) is a contraindication to \textbf{methylprednisolone}
\end{enumerate}

\noindent These features allow LeafAI to reason upon fairly complex non-specific criteria.

\subsection*{Query generation using semantic metadata mapping}

To enable data model-agnostic query generation, we leveraged a subset of codes within the UMLS in what we define as a semantic metadata mapping, or SMM. An includes a listing of available databases, tables, columns, and so on within a given database schema. Critically, these database artifacts are "tagged" using UMLS concepts. An example of this can be seen in Figure \ref{fig_leafai_smm}, which shows strategies by which a given criterion can be used to generate schema-specific queries by leveraging different SMMs. In cases where the LeafAI query engine finds more than one means of querying a concept (e.g., two SQL tables for diagnosis codes), the queries are combined in a UNION statement.

\begin{figure}[h]
  \includegraphics[scale=0.47]{figures/leafai_smm.pdf}  
\caption{The LeafAI query engine's SQL query generation process using two hypothetical database schema to generate queries for platelet counts (shown in logical form after normalization). This example illustrates the flexibility of LeafAI's semantic metadata mapping system (represented here in JSON format) in adapting to virtually any data model. On the left, "Tall Table Structure", platelet counts must be filtered from within a general purpose "labs" table. The LeafAI KB recognizes that labs may be stored as LOINC codes, and the corresponding SMM indicates that records in this table can be filtered to LOINC values. On the right, "Pivoted Table Structure", platelet counts are stored as a specific column in a "complete\_blood\_counts" table, and thus can be directly queried without further filtering. Additional metadata, columns, tables, types and so on needed in SMMs are omitted for brevity.}
\label{fig_leafai_smm}
\end{figure}

\subsection*{Evaluation}

An NLP-based system for finding patients based on eligibility criteria should be reasonably expected to find many or most patients enrolled in a real clinical trial - with the assumption that patients enrolled in those trials correctly met the necessary criteria as determined by study investigators. While there are caveats to this approach (for example, certain structured diagnosis codes may be missing for some patients, etc.), we aimed to establish a new baseline by which tools such as ours are evaluated in their ability to handle real-world eligibility criteria and clinical data.

We compared LeafAI's results to that of a human database programmer experienced in the use of clinical databases and data extraction. Our evaluation was performed as follows:

\begin{enumerate}
    \item We extracted metadata on 165 clinical trials from our EHR between January 2010 and December 2021 where at least 10 patients were indicated as enrolled and not withdrawn and the total number of raw lines within the eligibility criteria (besides the phrases "Inclusion Criteria" and "Exclusion Criteria") were less than or equal to 30.
    \item By manual review, we excluded 41 trials with multiple sub-groups, as it would not be possible to know which eligibility criteria applied to which sub-group of enrolled patients.
    \item Using the "condition" field for each trial within metadata from \url{https://clinicaltrials.gov}, we filtered and grouped the remaining 124 trials into only those related to predetermined groups: Cardiology, COVID-19, Crohn's Disease, Multiple Sclerosis, Diabetes Mellitus, Hepatitis C, and Oncology. 
    \item We randomly chose 1 trial from each group, with the exception of Oncology, where we chose 2 trials.
    \item Both the LeafAI query engine and human programmer created queries to find patients for each eligibility criteria, which we executed on an OMOP database derived from our EHR of our institution's entire research-eligible patient population. 
    \item In order to ensure results returned would be limited to only data available during the time of each trial, we replaced references to the SQL function for generating a current timestamp (\textit{GETDATE()}) with that of each trial's end date, and similarly replaced OMOP table references with SQL views filtering data to only that existing prior to the end of a trial.
    \item The human programmer was instructed to (1) ignore criteria which cannot be computed, (2) make a best-effort to reason upon non-specific criteria (e.g., symptoms for a condition), (3) not check whether patients found by a human query enrolled within a trial, and (4) skip criteria which cause an overall query to find no eligible patients. 
    \item As with LeafAI, the human-written SQL scripts also had any SQL \textit{GETDATE()} function references replaced with that of the end date for each trial.
\end{enumerate}

\end{document}