\documentclass[../main.tex]{subfiles}

\begin{document}

\subsection{Aim 2: Query Generation Methods Development and Evaluation}

In Aim 2 we focus on development and evaluation of methods for query generation and reasoning on eligibility criteria. We divide this aim into two sub-aims:

\begin{enumerate}
    \item Development of a gold standard eligibility criteria logical forms corpus.
    \item Creation of methods for query generation utilizing the corpus in (1)
\end{enumerate}

\subsubsection{SubAim 1: Creation of a Criteria to Logical Form Gold Standard}

Query generation for LeafAI was originally envisioned as a system which generates queries using rules executed upon predicted named entities and relations in the form of graphs. In practice, we found this system to be difficult to manage given the need for ever more rules, as well as "brittle" given new criteria structures the system had not seen before, which in turn required the creation of additional rules.

As a solution to this, we instead explored transforming original eligibility criteria into intermediate "logical form" representations, then generating SQL statements from the parsed logical forms. Intermediate representations (IRs) have a long history in computer science and NLP. IRs remove "noise" unnecessary to a given task and more closely represent underlying semantics while being agnostic to particulars of a final executed form (see Herzig \textit{et al} \cite{herzig2021unlocking} for an examination of IR-based SQL generation approaches). In related work, Roberts and Demner-Fushman \cite{roberts2016annotating} proposed a representation of questions on EHR databases using a comparatively compact but flexible format using first order logic expressions, for example, representing "Is she wheezing this morning?" as

\begin{quote}
    \centering
    $\delta( \lambda x.has\_problem(x, C0043144, status) \wedge time\_within(x, \mathrm{"this\ morning"}))$
\end{quote}

\noindent This style of representation is highly generalizable, but also difficult to translate directly into SQL statements as multiple predicates (e.g., \textit{has\_problem} and \textit{time\_within}) may correspond to a variable number of SQL statements, depending on context.

We thus chose a similar IR (hereafter simply "logical form") as proposed by Roberts and Demner-Fushman but closely more resembling a nested functional structure in programming languages such as Python or JavaScript. A criterion such as "Diabetic women and men over age 65" would be represented by our logical forms as

\begin{quote}
$intersect( \\
    \mathrm{\ \ \ \ }cond("Diabetic"), \\
    \mathrm{\ \ \ \ }union(female(), male()),\\
    \mathrm{\ \ \ \ }age().num\_filter(eq(op(GT), val("65"))) \\
)$
\end{quote}

We named the corpus produced from this sub-aim the Leaf Logical Forms (LLF) corpus. We developed annotation guidelines for the LLF corpus using a simplification of entities and relations from the preceding LCT corpus discussed in Aim 1. Generally speaking, LCT \textit{entities} correspond to logical form \textit{functions}, while LCT \textit{relations} correspond to logical form \textit{predicates}. For example, the LCT \textit{Condition} entity has a corresponding \textit{cond()} function, while the \textit{Num-Filter} relation has a corresponding \textit{.num\_filter()} method. The LLF annotation guidelines can be found at \url{https://github.com/ndobb/clinical-trials-seq2seq-annotation/wiki}.

We also hypothesized that the performance of predicting logical forms could likely be improved by replacing "raw" tokens in each eligibility criteria with corresponding functions names derived from named entities from the LCT corpus. For example, given a raw input of \\

'\textit{Diabetics who smoke}', \\

\noindent we would replace the name entities for "Diabetics" and "smoke", outputting \\ 

'\textit{cond("Diabetics") who obs("smoke")}' \\

\noindent using \textit{Condition} and \textit{Observation} named entity annotations in the LCT corpus. We call this substituted text an "augmented" eligibility criteria. The augmented criteria syntax reshapes named entities to more closely resemble expected logical form syntax and allows us to leverage the LCT corpus for logical form transformation.

Creation and annotation of the LLF corpus proceeded in the following steps:

\begin{enumerate}
    \item We randomly chose 2,000 lines of eligibility criteria from the LCT corpus, limited to only criteria which included at least one named entity and which were not annotated as hypothetical criteria.
    \item  Each annotation file consisted of an original "raw" eligibility criteria (line 1), an augmented eligibility criteria derived programmatically using the LCT corpus (line 3), and an (initially blank) expected logical form equivalent to annotate (line 5).
    \item 3 biomedical informatics graduate students met weekly for 2 months to review annotations. Annotators were initially trained on 20 triple-annotated training annotations. 
    \item After training, each annotator was assigned a batch of 100 sentences (one per file) and tasked with writing a logical form version of each.
    \item After each batch was completed, we executed a quality control script to parse each logical form annotation to ensure consistency. Any syntax errors were reported to and corrected by the annotators.
    \item Annotators received additional batches of files to annotate until all 2,000 single-annotated annotations had been completed.
\end{enumerate}

After annotations were completed, we experimented with predicting logical forms by fine-tuning T5 \cite{raffel2020exploring} Seq2Seq models. Following earlier work by Einolghozatic \textit{et al} \cite{einolghozati2019improving} and Rongali \textit{et al} \cite{rongali2020don} on task-oriented dialog semantic parsing structures, we also experimented with various alternative input-output syntax styles with the same semantic information as our "Standard" logical forms:

\begin{enumerate}
    \item \textbf{Pointer style}. Rongali \textit{et al} found that replacing input tokens with the token "$@ptr_{index}$", where \textit{index} corresponds to a token's sequential position in the input text, to improve performance. We modified this approach by using the sequential position of the quoted text in our input criteria instead.
    \item \textbf{Shift-Reduce}. Einolghozatic \textit{et al} \cite{einolghozati2019improving} used square brackets before predicates instead of parentheses and blank spaces instead of commas. We followed Rongali \textit{et al's} suggestion to add a trailing repeat of predicate names to improve performance.
\end{enumerate}

We used a 70/20/10 train/test/validation split of the LFF corpus to fine-tune the T5$_{base}$ model using combinations of these formats. Example inputs, outputs, and training results are shown in Table \ref{aim2_tbl_corpus}. 

\begin{table}[h!]
    \footnotesize
    \centering
    \input{Tables/Aim2/aim2_corpus}
    \caption{Example inputs and logical form syntax styles with fine-tuning performance results using the T5$_{base}$ model.}
    \label{aim2_tbl_corpus}
\end{table} 

We found that our "Standard" logical forms achieved the highest performance using both BLEU \cite{lin2004rouge} and ROUGE-L \cite{ callison2006re} scores, two commonly used metrics in measuring Seq2Seq performance. Replacing raw tokens with function names corresponding to named entities also significantly improved performance (+14.7\%), demonstrating that using the LCT corpus (for named entities) and LFF corpus (for logical forms) in tandem can achieve relatively high performance (> 93\% BLEU score) for this task.

\subsubsection{SubAim 2: Query Generation}
In sub-aim 2, we developed the LeafAI query engine, an application capable of generating database queries for cohort discovery from free-text eligibility descriptions. This sub-aim contributes the following:

\begin{enumerate}
    \item{A novel database schema annotation and mapping method to enable data model-agnostic query generation from natural language.}
    \item{Methods for transforming and leveraging intermediate logical representations of eligibility criteria.}
    \item{Methods for dynamically reasoning upon non-specific criteria using an integrated knowledge base of biomedical concepts.}
\end{enumerate}

\subsubsection{System Architecture}

The LeafAI query engine was designed using a modular, micro service-based architecture with a central API (Application Program Interface) which orchestrates end-to-end query generation. Inter-module communication is performed using gRPC \cite{grpc}, a robust open-source remote procedure call framework which enables language-agnostic service integration. This allows individual modules to be implemented (and substituted) in programming languages and using libraries well-suited to a given task. A diagram of the LeafAI query engine architecture is shown in Figure \ref{aim2_fig_leafai_architecture}. 

\begin{figure}[h]
  \includegraphics[scale=0.65]{Figures/Aim2/aim2_leafai_architecture.pdf}  
\caption{LeafAI query architecture. Inter-module communication is performed using the gRPC framework. Individual modules are deployed as Docker \cite{docker} containers and communicate solely with the central API, which orchestrates query generation and handles query generation requests.}
\label{aim2_fig_leafai_architecture}
\end{figure}

At a high level, query generation is performed in the following steps:

\begin{enumerate}
    \item{A query request is received by the API in the form of inclusion and exclusion criteria as free-text strings.}
    \item{The input texts are tokenized and named entity recognition is performed to determine spans of text representing conditions, procedures, and so on.}
    \item{Relation extraction is performed to determine relations between named entities. Any entities found with a hypothetical \textit{Assertion} relation (e.g., "could become pregnant") are excluded.}
    \item{The input eligibility criteria are transformed by replacing spans of "raw" text with logical form names as in sub-aim 1. The resulting augmented criteria are inputted into our fine-tuned T5 model, which outputs a predicted logical form string.}
    \item{A logical form interpreter module implemented as a recursive descent parser \cite{johnstone1998generalised} reads the logical form string and instantiates it as an abstract syntax tree (AST) of nested in-memory logical form objects.}
    \item{"Named" logical form objects (i.e., specified with quoted text, such as \textit{lab("hemoglobin A1c")}) are normalized into one or more corresponding UMLS concepts.}
    \item{Working recursively inside-to-outside the AST structure, each logical form object calls a \textit{Reason()} method which executes various rules depending on context.}
    \item{Each reasoning rule is performed as one or more pre-defined SPARQL queries to the knowledge base (KB), concept by concept.}
    \item{The normalized, reasoned, logical form AST is thus a nested structure of UMLS concepts. Each AST criterion is mapped to zero or more corresponding entries in the semantic metadata mapping (SMM).}
    \item{The final mapped AST object is transformed into a series of database queries, one per line of eligibility criteria text. The output SQL query can either be executed directly on a database or returned to the API caller.}
\end{enumerate}

\noindent Figure \ref{aim2_fig_leafai_querygen} illustrates an example of this process. Next we examine these steps in detail. \\

\begin{figure}[h]
  \includegraphics[scale=0.52]{Figures/Aim2/aim2_leafai_flow.pdf}  
\caption{LeafAI query generation processes}
\label{aim2_fig_leafai_querygen}
\end{figure}

\noindent \textbf{Named entity recognition and relation extraction} \\
We used the Leaf Clinical Trials (LCT) corpus \cite{dobbins2022leaf} to train two BERT-based \cite{devlin2018bert} NER extractors, one each for LCT general- and fine-grained-entities (see \cite{dobbins2022leaf} for more information on LCT entity types). Next, we perform relation extraction between named entity pairs similarly using a BERT-based model also trained on the LCT corpus. \\

\noindent \textbf{Logical form transformation} \\
As discussed, we leverage a fine-tuned T5 model for predicting logical forms. As inputs to the T5 model we use the original eligibility criteria with named entity spans replaced by logical form functions. For example, '\textit{Diagnosed with diabetes}' would become \textit{Diagnosed with cond("diabetes")}. After prediction, the output logical form string is then instantiated into an AST of nested in-memory logical form objects using a recursive descent parser within the API. \\

\noindent \textbf{Concept normalization} \\
We consider a logical form "named" if it contains a free-text value surrounding by quotes. For logical forms besides laboratory values, we used MetaMap \cite{aronson2001effective}. Normalization using MetaMap can often result in high recall but low precision, as MetaMap has no NER component and often finds multiple matching potential candidate concepts. To improve normalization precision, we employ two strategies. First, we compare term frequency-inverse document frequency (tf-idf) on MetaMap predictions, dropping UMLS concepts whose matched spans have a tf-idf score lower than that of unmatched spans in a given named entity. For example, for the string "covid-19 infection", MetaMap predicts both "COVID-19" (C5203670) as well as several concepts related to general infections. Using our tf-idf strategy removes the erroneous infection concepts. Next, our NER component also us to further improve precision by filtering the predicted UMLS concepts to only those of specific semantic types. For example, we limit condition concepts to only those which include semantic types of signs or symptoms, diseases or syndromes, and so on. 

Laboratory values also present a particular challenge, as LeafAI requires lab concepts to have directly associated LOINC codes, while MetaMap typically normalizes lab test strings to UMLS concepts of semantic type "laboratory test or finding", but which do not have direct mappings to LOINC. For example, a search for "platelet count" with MetaMap returns the concept "Platelet Count Measurement" (C0032181), but not the needed concept of "Platelet \# Bld Auto" (C0362994). Thus similar to Lee and Uzuner with medications \cite{lee2020normalizing}, we trained a BERT model specifically for normalization of lab tests. \\

\noindent \textbf{Reasoning using an integrated knowledge base} \\
\noindent For reasoning and derivation of ICD-10, LOINC, and other codes for UMLS concepts, we designed a knowledge base (KB) accessible via SPARQL queries and stored as RDF triples. The core of our KB is the UMLS, derived using a variation of techniques created for ontologies in BioPortal \cite{noy2009bioportal}. To further augment the UMLS, we mapped and integrated the Disease Ontology \cite{schriml2012disease}, Symptom Ontology \cite{sayers2010database}, COVID-19 Ontology \cite{sargsyan2020covid}, Potential Drug-Drug Interactions \cite{ayvaz2015toward}, LOINC2HPO \cite{zhang2019semantic}, and the Disease-Symptom Knowledge Base \cite{wang2008automated}. We then developed SPARQL queries parameterized by UMLS concepts for various scenarios which leveraged our KB, such as contraindications to treatments, symptoms of diseases, and so on. Using LOINC2HPO mappings further allows us to infer phenotypes by lab test results rather than only ICD-10 or SNOMED codes. For example, given the logical form \textit{cond("Hypercalcemia"}, our system will additionally search for abnormally high lab results of calcium [mass/volume] (LOINC: 17861-6).

Together our KB, nested logical forms, and inside-to-outside normalization methods enable "multi-hop" reasoning on eligibility criteria over several steps. For example, given the non-specific criterion "Contraindications to drugs for conditions which affect respiratory function", our system successfully reasons that (among other results),

\begin{enumerate}
    \item \textbf{Asthma} causes changes to \textbf{respiratory function}
    \item \textbf{Methylprednisolone} can be used to treat \textbf{asthma}
    \item \textbf{Mycosis} (fungal infection) is a contraindication to \textbf{methylprednisolone}
\end{enumerate}

\noindent These features allow LeafAI to reason upon fairly complex non-specific criteria. \\

\noindent \textbf{Query generation using semantic metadata mapping} \\
To allow for data model-agnostic query generation, we leveraged a subset of codes within the UMLS in what we define as a semantic metadata mapping, or SMM. An SMM is described using JSON, and includes a listing of available databases, tables, columns, and so on. Critically, these database artifacts are "tagged" using UMLS concepts. An example of this can be seen in Figure \ref{aim2_fig_leafai_smm}, which shows strategies by which a given criterion can be used to generate schema-specific queries by leveraging different SMMs. In cases where the LeafAI query engine finds more than one means of querying a concept (e.g., two SQL tables for diagnosis codes), the queries are combined in a UNION statement.

\begin{figure}[h]
  \includegraphics[scale=0.56]{Figures/Aim2/aim2_leafai_smm.pdf}  
\caption{The LeafAI query engine's SQL query generation process using two hypothetical database schema to generate queries for platelet counts (shown in logical form after normalization). This example illustrates the flexibility of LeafAI's semantic metadata mapping system in adapting to virtually any data model. On the left, "Tall Table Structure", platelet counts must be filtered from within a general purpose "labs" table. The LeafAI KB recognizes that labs may be stored as LOINC codes, and the corresponding SMM indicates that records in this table can be filtered to LOINC values. On the right, "Pivoted Table Structure", platelet counts are stored as a specific column in a "complete\_blood\_counts" table, and thus can be directly queried without further filtering. Additional metadata, columns, tables, types and so on needed in SMMs are omitted for brevity.}
\label{aim2_fig_leafai_smm}
\end{figure}

\subsubsection{Evaluation}
An NLP-based system for finding patients based on eligibility criteria should be reasonably expected to find many or most patients enrolled in a real clinical trial - with the assumption that patients enrolled in said trial correctly met the necessary criteria as determined by study investigators. While there are caveats to this approach (for example, certain structured data may be missing for some patients), we aimed to establish a new baseline by which tools such as ours are evaluated in their ability to handle real-world eligibility criteria and clinical data.

For comparison, we also used the latest version of Criteria2Query (version 2.4) and a human database programmer experienced with clinical databases and data extraction. Our evaluation was performed as follows:

\begin{enumerate}
    \item We extracted metadata on 165 clinical trials from our EHR between January 2017 and December 2021 where at least 10 patients were indicated as enrolled and not withdrawn and the total number of raw lines within the eligibility criteria (besides the phrases "Inclusion Criteria" and "Exclusion Criteria") were less than or equal to 30. We excluded 41 trials with multiple sub-groups, as it would not be possible to know which eligibility criteria applied to which sub-group of enrolled patients.
    \item Using the "condition" field for each trial within metadata from \url{https://clinicaltrials.gov}, we filtered and grouped the remaining 124 trials into only those related to predetermined groups: Cardiology, COVID-19, Crohn's Disease, Multiple Sclerosis, Diabetes Mellitus, Hepatitis C, and Oncology. 
    \item We randomly chose 1 trial from each group, with the exception of Oncology, where we chose 2 trials.
    \item The human programmer was provided the eligibility criteria for each of the 8 selected trials and instructed to (1) ignore criteria which cannot be computed, (2) make a reasonable effort to reason upon non-specific criteria (e.g., symptoms for a condition), (3) not check whether patients found by a query enrolled within a trial, and (4) skip criteria which cause an overall query to find no eligible patients, as they typically indicate missing data.
    \item We generated queries using both the LeafAI query engine and Criteria2Query, modifying the output SQL of each to output the number of matched patients at each step (for LeafAI) or at the end of the script (for Criteria2Query). 
    \item In order to ensure results returned would be limited to only data available during the time of each trial, for each system we (1) replaced references to the SQL function for generating a current timestamp with that of each trial's end date, and similarly replaced OMOP table references with SQL views filtering data to only that existing prior to the end of a trial.
\end{enumerate}

\subsubsection{Results}

Current results of our experiments are shown in Table \ref{aim2_tbl_results}. 

\begin{table}[h!]
    \footnotesize
    \centering
    \input{Tables/Aim2/aim2_leafai_results}
    \caption{Statistics for each clinical trial evaluated by the LeafAI query engine, human programmer, and Criteria2Query v2.4. The number of enrolled and matched patients were determined by cross-matching enrollments listed within our EHR. The "\# Crit." column refers to the number of lines of eligibility criteria which were not empty and did not contain the phrases "Inclusion Criteria" or "Exclusion Criteria".}
    \label{aim2_tbl_results}
\end{table} 

While our analysis of the queries from each system is ongoing, we found that a total of 284 of the 427 (66\%) patients enrolled across the 8 trials were successfully matched by LeafAI, of a total of 27,225 patients predicted to be eligible. Criteria2Query is generally recognized as the current state-of-the-art in this task \cite{tseo2020information, zhang2020deepenroll, gao2020compose}, but surprisingly found eligible patients only in one trial, for chronic lymphocytic leukemia (CLL). LeafAI found over 40\% of enrolled patients in 5 of 8 (62\%) of trials and zero patients in the remaining 3 trials. Figure \ref{aim2_fig_leafai_results_detail} shows matched patients from each trial's executed queries tracked step-by-step.

\begin{figure}[h!]
  \includegraphics[scale=0.47]{Figures/Aim2/aim2_leafai_detail_results_longitudinal.png}  
  \caption{Longitudinal results listing results of patients found at each step in the query process for each trial. The blue line indicates \% Recall (left-most Y axis) while the gray line indicates the number of eligible patients found (right-most Y axis). The X axis represents the line number within the free-text eligibility criteria. Dots indicate that the LeafAI query engine executed an eligibility criterion query.}
\label{aim2_fig_leafai_results_detail}
\end{figure}

While LeafAI executed queries for Crohn's Disease (NCT03782376) and Type 1 Diabetes Mellitus (NCT03335371) ultimately matched zero patients, as Figure \ref{aim2_fig_leafai_results_detail} shows, initially in both trials LeafAI matched at least have of the enrolled patients, who were subsequently were found ineligible in later criteria. Preliminary error analysis also reveals certain interesting findings. For example, line 12 of trial NCT02786537 for Hepatitis C excludes "Child Pugh (CTP) B or C Cirrhosis)", while 11 of the 42 enrolled patients had diagnosis codes for cirrhosis, and were thus excluded by LeafAI.

\subsubsection{Limitations and Future Work}

To the best of our knowledge, the query generation and associated methods of LeafAI are the current state-of-the-art for the task of cohort discovery using a natural language interface. Nevertheless our system has numerous limitations. First, while we evaluated our query generation methods using a random sample of 8 actual clinical trials, those trials are a small subset of actual trials performed at the University of Washington and thus performance may vary significantly if our evaluation was expanded to a greater number of trials and disease domains. Second, while capable of reasoning across a large number of different scenarios and diseases, our KB and reasoning module use rules and pre-determined queries which likely fall short and fail to capture correct results in many cases. Moreover, our reasoning module often fails to capture in the nuance of many clinical concepts. For example, in Figure \ref{aim2_fig_leafai_querygen}, renal insufficiency and Dolutegravir are shown as contraindications to Metformin, but in reality for many patients those may not be absolute contraindications, depending on other health factors. Last, while our logical forms representation is uniquely flexible and demonstrably capable of representing many eligibility criteria, there are nonetheless likely many cases where our representation does not fully capture the semantics and nuance intended within certain criteria.

In future work, we intend to explore extending our logical form representation for general-purpose question-answering. Leveraging the outer-most functions introduced by Roberts and Demner-Fushman \cite{roberts2016annotating} such as \textit{latest()}, $\lambda$(), or \textit{min()}, our logical forms could be "wrapped" such that LFF logical forms are used for data retrieval while Roberts and Demner-Fushman's functions can be executed to analyze returned data and answer a question. For example, the question 

"\textit{Did the patient’s temperature exceed 38C in the last 48 hrs?}" \\

\noindent could be represented as \\

\begin{quote}
$\lambda( \\
    \mathrm{\ \ \ \ }measurement("temperature") \\
    \mathrm{\ \ \ \ \ \ \ }.num\_filter(eq(op(GT), val("38"), unit("C")))\\
    \mathrm{\ \ \ \ \ \ \ }.temporality(eq(op(LTEQ), val("48"), unit(HOUR))) \\
)$
\end{quote}

While general-purpose question-answering is not a specific aim of this project, the methods we have developed have potential to be a foundation for exciting future research directions. 

\end{document}